{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram model of Text8 data\n",
    "\n",
    "In this notebook, I apply the code in this repository to train word embeddings on a skip-gram model of the Text8 data (a few gigabytes of pre-pruned text from Wikipedia of 2006). This is essentially my own and extended implementation of an exercise in Udacity's Deep Learning course by Google. The results will, thus, also be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import tools as t\n",
    "from batchgen import ContinuousBatchGenerator\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data first. The .zip file can be downloaded from here if needed: http://mattmahoney.net/dc/textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text8.zip\"\n",
    "with zipfile.ZipFile(filename) as f:\n",
    "    data = f.read(f.namelist()[0]).decode().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tools.DataContainer` helps dealing with operations related to manipulating the data. We use it to convert the data into a list of IDs which each correspond to a unique word. Words that are too rare are replaced with ID=0. The `unknown_token` is only needed if we want to convert back from IDs to words at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacontainer = t.DataContainer(data, maxwords=100000,\n",
    "                                unknown_token=\"_UNKNOWN_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator we choose here takes a continuous list of data and creates an iterator that produces batches of data to the model. We use here the skip-gram model which tries to predict a random single word within a given window from a given word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ContinuousBatchGenerator(datacontainer.data, 4, 2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model itself is a rather simple part of the program. It consists of only an embedding lookup table and a single densely connected layer with a softmax activation function. For the skip-gram model, rather long training times are needed to produce good results especially for less common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./weights/skip-gram-text8.save\n",
      "Weights and embeddings loaded from ./weights/skip-gram-text8.save\n",
      "Starting training with 100000 steps.\n",
      "Step: 2000: loss = 5.449187862932682\n",
      "Step: 4000: loss = 5.500022613659501\n",
      "Step: 6000: loss = 5.471134647727013\n",
      "Step: 8000: loss = 5.584422599673271\n",
      "Step: 10000: loss = 5.435981060504913\n",
      "Step: 12000: loss = 5.484842420578003\n",
      "Step: 14000: loss = 5.5697263346910475\n",
      "Step: 16000: loss = 5.311738821268082\n",
      "Step: 18000: loss = 5.510166151165962\n",
      "Step: 20000: loss = 5.537298298954964\n",
      "Step: 22000: loss = 5.570484763622284\n",
      "Step: 24000: loss = 5.652584486663342\n",
      "Step: 26000: loss = 5.6008817142248155\n",
      "Step: 28000: loss = 5.541498960852623\n",
      "Step: 30000: loss = 5.4085981355905535\n",
      "Step: 32000: loss = 5.407236380815506\n",
      "Step: 34000: loss = 5.35655417740345\n",
      "Step: 36000: loss = 5.563209484457969\n",
      "Step: 38000: loss = 5.4056957747936245\n",
      "Step: 40000: loss = 5.012357564091682\n",
      "Step: 42000: loss = 5.000904550552368\n",
      "Step: 44000: loss = 5.4497411014735695\n",
      "Step: 46000: loss = 5.423235018730163\n",
      "Step: 48000: loss = 5.468664083480835\n",
      "Step: 50000: loss = 5.472837700247765\n",
      "Step: 52000: loss = 5.412716146409512\n",
      "Step: 54000: loss = 5.467554015556641\n",
      "Step: 56000: loss = 5.394593871414662\n",
      "Step: 58000: loss = 5.510401006221771\n",
      "Step: 60000: loss = 5.266548453271389\n",
      "Step: 62000: loss = 5.457425645470619\n",
      "Step: 64000: loss = 5.55130493414402\n",
      "Step: 66000: loss = 5.505630057096481\n",
      "Step: 68000: loss = 5.3323810613155365\n",
      "Step: 70000: loss = 5.453346574544907\n",
      "Step: 72000: loss = 5.54640097630024\n",
      "Step: 74000: loss = 5.542662635684013\n",
      "Step: 76000: loss = 5.533235159516335\n",
      "Step: 78000: loss = 5.450375245034695\n",
      "Step: 80000: loss = 5.338549585223198\n",
      "Step: 82000: loss = 5.416313498854637\n",
      "Step: 84000: loss = 5.32408688140288\n",
      "Step: 86000: loss = 5.488433117866516\n",
      "Step: 88000: loss = 5.376957424402237\n",
      "Step: 90000: loss = 5.480057183265686\n",
      "Step: 92000: loss = 5.494182734370232\n",
      "Step: 94000: loss = 5.240415092527867\n",
      "Step: 96000: loss = 5.466938492655754\n",
      "Step: 98000: loss = 5.3720833942890165\n"
     ]
    }
   ],
   "source": [
    "model = W2VModel(100001, 1000, save_path=\"./weights/skip-gram-text8.save\")\n",
    "model.train(datagen, 500001)\n",
    "datacontainer.add_embeddings(model.final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quality of the results by checking the nearest neighbors of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words closest to 'two' are:\n",
      "['zero', 'five', 'four', 'one', 'three']\n",
      "Words closest to 'january' are:\n",
      "['july', 'february', 'october', 'april', 'december']\n",
      "Words closest to 'russia' are:\n",
      "['italy', 'spain', 'germany', 'china', 'bulgaria']\n",
      "Words closest to 'banana' are:\n",
      "['pathos', 'distinctive', 'sadist', 'berman', 'seljuk']\n",
      "Words closest to 'mouse' are:\n",
      "['sources', 'call', 'file', 'playing', 'uses']\n"
     ]
    }
   ],
   "source": [
    "some_words = ['two', 'january', 'russia', 'banana', 'mouse']\n",
    "for word in some_words:\n",
    "    print(\"Words closest to '{}' are:\".format(word))\n",
    "    print(datacontainer.closest_to(word)[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
